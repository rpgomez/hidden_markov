{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "===\n",
    "\n",
    "I'm going to derive the equations for computing probabilities on hidden states given observed data for [Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries:\n",
    "\n",
    "I'm going to define my variables here:\n",
    "\n",
    "* $(Y_t)_{t=0}^{T}$ is the observed sequence\n",
    "* $(X_t)_{t=0}^T$ is the hidden sequence.\n",
    "* $A_{i\\rightarrow j} := Pr(X_{t+1} = j| X_t = i)$ is the transition matrix from one hidden state to another.\n",
    "* $B_i(j) := Pr(Y = j | X = i)$ is the probability distribution of the observed value as a function of the hidden state.\n",
    "* $\\pi(i) := Pr(X_0 = i)$ is the initial probability distribution on the hidden state.\n",
    "\n",
    "\n",
    "Here are the assumptions based on the Markovity of the hidden sequence:\n",
    "\n",
    "$$Pr(X_{t+1} = j| Y_0, \\ldots, Y_t, X_0=s_0,X_1=s_1, \\ldots,X_t = s_t) = Pr(X_{t+1} = j | X_t = s_t)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Pr((Y_s)_{s=t+1}^T | (Y_s)_{s=0}^t,X_0 = s_0,\\ldots,X_t=s_t) = Pr((Y_s)_{s=t}^T | X_t=s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goals:\n",
    "To be able to compute:\n",
    "    \n",
    "* $Pr(X_t = i | (Y_s)_{s=0}^T)$\n",
    "* $Pr(X_{t} =i, X_{t+1} = j | (Y_s)_{s=0}^T)$\n",
    "* Find a better model $\\pi, A, B$ that explains the observed data.\n",
    "\n",
    "To do so I'll need to do some recursive computations.\n",
    "\n",
    "We begin:\n",
    "\n",
    "## Define: \n",
    "\n",
    "* $\\gamma_{t}(i) := Pr((Y_s)_{s=0}^T, X_t = i)$\n",
    "* $\\alpha_t(i) := Pr((Y_s)_{s=0}^t, X_t = i)$\n",
    "* $\\beta_{t}(i) := Pr((Y_s)_{s=t+1}^T| X_t=i)$\n",
    "\n",
    "## Relationship between $\\alpha, \\beta, \\gamma$:\n",
    "Here's the relationship between $\\alpha_t, \\beta_t, \\gamma_t$ :\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma_{t}(i) &:= Pr((Y_s)_{s=0}^T, X_t = i) \\\\\n",
    " &= Pr((Y_s)_{t+1}^T | X_t = i, (Y_s)_{s=0}^t ) Pr(X_t = i, (Y_s)_{s=0}^t )\\\\\n",
    " &= Pr((Y_s)_{t+1}^T | X_t = i) Pr(X_t = i, (Y_s)_{s=0}^t )\\\\\n",
    " &= \\beta_t(i)\\alpha_t(i) \\\\\n",
    " \\gamma_{t}(i) &= \\beta_t(i)\\alpha_t(i) \\\\\n",
    "\\end{align}  \n",
    "\n",
    "Since $\\gamma_T(i) \\equiv \\alpha_T(i)$ then $\\beta_T(i) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the relationship between $\\alpha_t, \\alpha_{t+1}$: \n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{t+1}(j) &= Pr((Y_s)_{s=0}^{t+1}, X_{t+1} = j) \\\\\n",
    "&= \\sum_i Pr( (Y_s)_{s=0}^{t+1}, X_{t+1} = j, X_t = i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}, X_{t+1} = j | (Y_s)_{s=0}^{t}, X_t = i)Pr( (Y_s)_{s=0}^{t}, X_t = i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}, X_{t+1} = j | X_t = i)\\alpha_t(i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}|X_{t+1} = j, X_t = i)Pr(X_{t+1} = j, X_t = i)\\alpha_t(i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}|X_{t+1} = j)A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "&= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\alpha_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the relationship between $\\beta_t$ and $\\beta_{t+1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_t(j) & = Pr((Y_s)_{s=t+1}^T| X_t=j) \\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+1}^T,X_{t+1} = i| X_t=j) \\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+2}^T|Y_{t+1},X_{t+1} = i, X_t=j) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+2}^T|X_{t+1} = i) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i \\beta_{t+1}(i) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)Pr(Y_{t+1}|X_{t+1} = i, X_t = j)Pr(X_{t+1} = i | X_t = j)\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)Pr(Y_{t+1}|X_{t+1} = i)A_{j\\rightarrow i}\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\beta_t(j) &= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Factors\n",
    "For any decent length observed sequence, the $\\alpha, \\beta, \\gamma$ sequences will quickly converge to 0 due to underflow issues in the finite precision arithmetic we have for computations.\n",
    "\n",
    "To avoid this underflow we instead compute conditional probabilities recursively which will not underflow as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional $\\alpha$\n",
    "\\begin{align}\n",
    "\\hat{\\alpha}_t(i) &:= Pr(X_t = i | Y_0,\\ldots, Y_t) \\\\\n",
    "&= \\frac{Pr(X_t = i, Y_0,\\ldots, Y_t)}{Pr(Y_0,\\ldots, Y_t)} \\\\\n",
    "&= \\frac{\\alpha_t(i)}{\\Lambda_t}, \\quad \\Lambda_t := Pr(Y_0,\\ldots, Y_t)\n",
    "\\end{align}\n",
    "$\\hat{\\alpha}_t$ satisfies the equation:\n",
    "\n",
    "$$\\sum_i \\hat{\\alpha}_t(i) = \\sum_i Pr(X_t = i | Y_0,\\ldots, Y_t) = 1$$\n",
    "In terms of the recursive computation of $\\alpha_{t+1}$  as a function of $\\alpha_t$ we have the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\Lambda_{t+1}\\hat{\\alpha}_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\Lambda_t\\hat{\\alpha}_t(i) \\\\\n",
    "\\hat{\\alpha}_{t+1}(j) &= \\frac{\\Lambda_{t}}{\\Lambda_{t+1}}\\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i) \\\\\n",
    "&= \\frac1{\\sigma_{t+1}} \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i), \\quad \\sigma_{t+1} := \\frac{\\Lambda_{t+1}}{\\Lambda_{t}}\n",
    "\\end{align}\n",
    "We define $\\sigma_0 := Pr(Y_0)$, and we can recover $\\sigma_{t+1}$ recursively from the condition that $\\sum_j \\hat{\\alpha}_{t+1}(j) = 1$ which implies:\n",
    "\n",
    "$$\\sigma_{t+1} = \\sum_j\\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)$$ \n",
    "\n",
    "and also:\n",
    "\n",
    "$$\\Lambda_t = \\prod_{s=0}^t \\sigma_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional $\\beta$\n",
    "\n",
    "We want to take advantage of how we defined $\\hat{\\alpha}$ so we can compute a $\\beta$ that does not underflow. Let's define $\\hat{\\gamma}_t(i)$ first:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\gamma}_t(i) &:= Pr(X_t = i | Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(X_t = i , Y_0,\\ldots, Y_T)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\alpha_t(i)\\beta_t(i)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\Lambda_t\\hat{\\alpha}_t(i)\\beta_t(i)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\hat{\\alpha}_t(i)\\beta_t(i)\\frac{\\Lambda_t}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\hat{\\alpha}_t(i)\\hat{\\beta}_t(i) \\\\\n",
    "\\end{align}\n",
    "Now we can define $\\hat{\\beta}_t(i)$:\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_t(i) &:= \\frac{\\Lambda_t}{Pr(Y_0,\\ldots, Y_T)}\\beta_t(i) \\\\\n",
    "&= \\frac{\\prod_{s=0}^t \\sigma_s}{\\prod_{s=0}^T \\sigma_s}\\beta_t(i)\\\\\n",
    "&= \\frac1{\\prod_{s=t+1}^T \\sigma_s}\\beta_t(i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Lambda_{T} = Pr(Y_0,\\ldots,Y_T)$ we can deduce that $\\hat{\\beta}_T(i) = \\beta_T(i) = 1$ \n",
    "\n",
    "Now we can take advantage of the recursion on $\\beta_t$ to compute $\\hat{\\beta}_t$ as a function of $\\hat{\\beta}_{t+1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_t(j) &= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\left(\\prod_{s=t+1}^T \\sigma_s\\right) \\hat{\\beta}_t(j) &= \\left(\\prod_{s=t+2}^T\\sigma_s\\right)\\sum_i  \\hat{\\beta}_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\hat{\\beta}_t(j) &=\\frac{1}{\\sigma_{t+1}}\\sum_i  \\hat{\\beta}_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the estimates of $A,B,\\pi$ based on the observations\n",
    "I want to apply [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) to improve the model for the observed data by updating the parameters $A,B,\\pi$. Some of the parameters are easy to re-estimate:\n",
    "\n",
    "$\\pi(i) := Pr(X_0 = i) $ gets updated to $ \\pi'(i) = Pr(X_0=i | Y_0,\\ldots, Y_T) = \\hat{\\gamma}_0(i)$\n",
    "\n",
    "$B_i(y) := Pr(Y=y|X = i)$ gets updated to \n",
    "\n",
    "\\begin{align}\n",
    "B'_i(y) &:= Pr(Y=y | X=i, Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(Y=y, X=i, Y_0,\\ldots, Y_T)}{Pr(X=i, Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(Y=y, X=i| Y_0,\\ldots, Y_T)Pr( Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)Pr( Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(Y=y, X=i| Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{\\frac1T\\sum_{t=0}^T Pr(X_t=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{\\frac1T\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\\n",
    "&= \\frac{\\frac1T\\sum_{t=0,Y_t=y}^T Pr(X=i| Y_0,\\ldots, Y_T)\\times 1}{\\frac1T\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\\n",
    "&= \\frac{\\sum_{t=0,Y_t=y}^T \\hat{\\gamma}_t(i)}{\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to determine how to update $A$:\n",
    "\n",
    "$A_{i\\rightarrow j} = Pr(X_{new} = j | X_{old} = i)$ updates to \n",
    "\n",
    "\\begin{align}\n",
    "A'_{i \\rightarrow j} &= Pr(X_{new} = j | X_{old} = i,Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(X_{new} = j, X_{old} = i,Y_0,\\ldots, Y_T)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(X_{t+1} = j, X_{t} = i,Y_0,\\ldots, Y_T)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(X_{t+1} = j, Y_{t+1},\\ldots Y_T| X_{t} = i,Y_0,\\ldots, Y_t)Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+1},\\ldots Y_T| X_{t+1} = j, X_{t} = i,Y_0,\\ldots, Y_t)Pr(X_{t+1} = j| X_{t} = i,Y_0,\\ldots, Y_t) Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+1},\\ldots Y_T| X_{t+1} = j)Pr(X_{t+1} = j| X_{t} = i) Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+2},\\ldots Y_T| X_{t+1} = j,Y_{t+1})Pr(Y_{t+1}|X_{t+1}=j)A_{i\\rightarrow j}\\alpha_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\beta_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s=t+2}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\left(\\prod_{s=0}^t \\sigma_s\\right)\\hat{\\alpha}_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\frac1{T-1}\\sum_{t=0}^{T-1}Pr(X_t = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\frac1{T-1}\\sum_{t=0}^{T-1}Pr(X_t = i|Y_0,\\ldots, Y_T)Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)\\prod_{s=0}^T\\sigma_s} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1}  1/\\sigma_{t+1} \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\log\\left(Pr(Y_0,\\ldots, Y_T| \\pi,A,B)\\right)$\n",
    "Here's how we compute the log of the probability $Pr(Y_0,\\ldots, Y_T| \\pi,A,B)$:\n",
    "\n",
    "\\begin{align}\n",
    "Pr(Y_0,\\ldots, Y_T| \\pi,A,B) &= \\prod_{t=0}^T \\sigma_t \\\\\n",
    "\\log\\left(Pr(Y_0,\\ldots, Y_T| \\pi,A,B)\\right) &= \\sum_{t=0}^T \\log\\left(\\sigma_t\\right) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of hidden states\n",
    "I can use [Wilks' theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem) to determine the number of hidden states as follows:\n",
    "\n",
    "1. Assume  $n$ hidden states. A model with $n$ hidden states and $S$\n",
    " observable states requires $n-1 + n(S-1) + n(n-1) = n(1 + S-1 + n -1) -1 = n(n + S -1) - 1 $ parameters.\n",
    "2. Use Baum-Welch's algorithm to hill climb to the best model it can find. \n",
    "3. Compute the log likelihood of the observed data , $S_{null} :=\\log Pr(Y_0,\\ldots, Y_T| \\pi, A,B)$.\n",
    "4. Assume now $n+1$ hidden states and as above recover the MLE parameters and  log likelihood, $S_{alternative}$.\n",
    "5. Compute the statistic $\\Lambda := 2\\times (S_{alternative} - S_{null})$. This statistic will be a sample from a Chi-squared distribution with $df = (n+1)(n+1 +S -1) - n(n+S-1) = n^2 + n + nS + S - n^2 - nS + n = 2n +S$ degrees of freedom. \n",
    "6. Compute the $p$ value $p:=Pr(\\xi \\ge \\Lambda)$. where $\\xi$ is a random variable of type Chi-Square distribution with $df = 2n +S$ degrees of freedom. If $p$ is below my threshold stick with alternative hypothesis (1 more hidden state) otherwise stop at $n$ hidden states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "I want to demonstrate that this derivation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random(N,S):\n",
    "    \"\"\"generates a random pi,A,B triplet for me.\"\"\"\n",
    "\n",
    "    pi = np.random.dirichlet(np.ones(N))\n",
    "    A  = np.random.dirichlet(np.ones(N),N).transpose()\n",
    "    B  = np.random.dirichlet(np.ones(S),N)\n",
    "\n",
    "    return pi,A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(pi,A,B,T = 100):\n",
    "    \"\"\"Generates an observed sequence of length T\"\"\"\n",
    "    N, S = B.shape\n",
    "    \n",
    "    # Generate hidden path\n",
    "    X_t = np.zeros(T,dtype=np.int32)\n",
    "    X_t[0] = np.argwhere(np.random.multinomial(1,pi,size=1).flatten()==1).flatten()[0]\n",
    "    for t in range(1,T):\n",
    "        X_t[t] = np.argwhere(np.random.multinomial(1,A[:,X_t[t-1]],size=1).flatten()==1).flatten()[0]\n",
    "\n",
    "    # Generate observed sequence\n",
    "    Y_t = np.zeros(T,dtype=np.int32)\n",
    "    for t in range(T):\n",
    "        Y_t[t] = np.argwhere(np.random.multinomial(1,B[X_t[t]],size=1).flatten()==1).flatten()[0]\n",
    "\n",
    "    return X_t,Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def alphapass(alpha,sigma,A,B,pi,observed):\n",
    "    \"\"\"\n",
    "    alpha(t,i) = Pr(hidden_t = i | obs_1,...,obs_t)\n",
    "    beta(t,i)  = Pr(obs_t+1,...,obs_T| hidden_t = i)*Pr(o_1,...,o_t)/Pr(o_1,...,o_T)\n",
    "    gamma(t,i)  = Pr(hidden_t = i | obs_1,...,obs_T)\n",
    "    A(j,i) == Pr( hidden i -> hidden j)\n",
    "    B(i,j) == Pr( observed j | hidden i)\n",
    "    \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    # initial iteration\n",
    "    alpha[0] = pi\n",
    "    alpha[0] = alpha[0]*B[:,observed[0]]\n",
    "    sigma[0] = alpha[0].sum()\n",
    "    alpha[0] /= sigma[0]\n",
    "\n",
    "    # now do the remainder\n",
    "    for t in range(1,T):\n",
    "        alpha[t] = np.dot(A,alpha[t-1])\n",
    "        alpha[t] *= B[:,observed[t]]\n",
    "        sigma[t] = alpha[t].sum()\n",
    "        alpha[t] /= sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def betapass(beta,sigma,A,B,observed):\n",
    "    \"\"\"\n",
    "    A(j,i) == Pr( hidden i -> hidden j)\n",
    "    B(i,j) == Pr( observed j | hidden i)\n",
    "    \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    At = A.transpose()\n",
    "\n",
    "    # initial iteration\n",
    "    beta[-1] = 1.\n",
    "\n",
    "    # now do the remainder\n",
    "    for t in range(T-1,0,-1):\n",
    "        beta[t-1] = np.dot(At,beta[t]*B[:,observed[t]])/sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def gammapass(alpha,beta,gamma):\n",
    "    gamma[:,:] = alpha*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def update_B(B,gamma, observed):\n",
    "    \"\"\" Updates belief on B \"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    B_new = np.zeros((N,S))\n",
    "\n",
    "    for t in range(T):\n",
    "        B_new[:,observed[t]] += gamma[t]\n",
    "\n",
    "    B_new = B_new/B_new.sum(axis=1).reshape(-1,1)\n",
    "    B[:] = B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi(pi,gamma):\n",
    "    \"\"\" Updates belief on pi \"\"\"\n",
    "    pi[:] = gamma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def digammapass(alpha,beta,gamma,observed,sigma,digamma,A,B):\n",
    "    \"\"\"\n",
    "    Computes an updated version of A based on the observed sequence of data.\n",
    "    \n",
    "    alpha(t,i) = Pr(hidden_t = i | obs_1,...,obs_t)\n",
    "    beta(t,i)  = Pr(obs_t+1,...,obs_T| hidden_t = i)*Pr(o_1,...,o_t)/Pr(o_1,...,o_T)\n",
    "    gamma(t,i)  = Pr(hidden_t = i | obs_1,...,obs_T)\n",
    "    A(j,i) = Pr( hidden i -> hidden j)\n",
    "    B(i,j) = Pr( observed j | hidden i)\n",
    "\n",
    "    Reestimates Pr(X=i -> X=j) via averaging\n",
    "    Pr(x_t=i,x_t+1=j|o_1,...,o_T) = alpha_t(i)A_jibeta_{t+1}(j)B(j,o_{t+1})\n",
    "    \"\"\"\n",
    "\n",
    "    T = alpha.shape[0]\n",
    "    N,S = B.shape\n",
    "    \n",
    "    digamma[:] = 0\n",
    "    digammatemp = np.zeros((N,N))\n",
    "    for t in range(0,T-1):\n",
    "        temp_i = (alpha[t]/sigma[t+1]).reshape(1,-1)*A\n",
    "        temp_j = (B[:,observed[t+1]]*beta[t+1]).reshape(-1,1)\n",
    "        digammatemp = temp_i*temp_j\n",
    "        digamma[:] += digammatemp\n",
    "\n",
    "    # rescale columns as \\sum_j A_{j,i} == 1\n",
    "    digamma[:] = digamma/digamma.sum(axis=0).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(alpha,beta,gamma, pi, A,B,observed,sigma):\n",
    "    \"\"\" Updates pi, A,B from observed data. \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    digamma = np.zeros((N,N))\n",
    "    digammapass(alpha, beta, gamma, observed,sigma,digamma, A,B)\n",
    "\n",
    "    update_B(B,gamma,observed)\n",
    "    update_pi(pi,gamma)\n",
    "\n",
    "    A[:] = digamma[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma):\n",
    "    \"\"\" computes alpha, beta, and gamma directly for me. \"\"\"\n",
    "\n",
    "    alphapass(alpha,sigma,A,B,pi,observed)\n",
    "    betapass(beta,sigma,A,B,observed)\n",
    "    gammapass(alpha,beta,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglikelihood(sigma):\n",
    "    \"\"\"Computes Pr(Y_0,...,Y_T| pi, A, B) from (sigma_t)\"\"\"\n",
    "    return log(sigma).sum()\n",
    "\n",
    "def compute_loglikelihood_parameters(pi,A,B,observed):\n",
    "    \"\"\"Computes Pr(Y_0,...,Y_T| pi, A, B) from (pi, A, B)\"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "    alpha = zeros((T,N))\n",
    "    beta  = zeros((T,N))\n",
    "    gamma = zeros((T,N))\n",
    "    sigma = zeros(T)\n",
    "    \n",
    "    gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "    score = compute_loglikelihood(sigma)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reestimate_parameters(A,B,pi,observed,\n",
    "                          halting_criteria=1e-6,debug=False):\n",
    "    \"\"\"re-estimates pi, A, and B from the observed sequence.\"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "    alpha = zeros((T,N))\n",
    "    beta  = zeros((T,N))\n",
    "    gamma = zeros((T,N))\n",
    "    sigma = zeros(T)\n",
    "    \n",
    "    current_score = -np.inf\n",
    "    gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "    new_score = compute_loglikelihood(sigma)\n",
    "    diff = new_score - current_score\n",
    "    while diff > halting_criteria:\n",
    "        current_score = new_score\n",
    "        update_parameters(alpha,beta,gamma, pi, A,B,observed,sigma)\n",
    "        gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "        new_score = compute_loglikelihood(sigma)\n",
    "        diff = new_score - current_score\n",
    "        if debug:\n",
    "            print(\"diff = \", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_number_of_hidden_states(Y_t,halting_criteria=1e-6,verbose=False):\n",
    "    \"\"\"Uses Wilks' theorem to determine the number of hidden states based on\n",
    "    the observed data (Y_t). It returns the number of hidden states\"\"\"\n",
    "    S = Y_t.max() + 1\n",
    "    N = 1\n",
    "    my_pi, my_A, my_B = make_random(N,S)\n",
    "\n",
    "    reestimate_parameters(my_A,my_B, my_pi,Y_t,halting_criteria=halting_criteria)\n",
    "    S_alternative =compute_loglikelihood_parameters(my_pi, my_A, my_B, Y_t)\n",
    "\n",
    "    p = 0.0\n",
    "    while p < 0.01:\n",
    "        df = 2*N + S\n",
    "        N += 1\n",
    "        my_pi, my_A, my_B = make_random(N,S)\n",
    "    \n",
    "        S_null = S_alternative\n",
    "        reestimate_parameters(my_A,my_B, my_pi,Y_t,halting_criteria=halting_criteria)\n",
    "        S_alternative =compute_loglikelihood_parameters(my_pi, my_A, my_B, Y_t)\n",
    "\n",
    "        model =st.chi2(df = df)\n",
    "        lam = 2*(S_alternative - S_null)\n",
    "        p = model.sf(lam)\n",
    "        if verbose:\n",
    "            print(\"N=\",N,\"df: \", df, \"Lamda: \", lam, \"p: \", p)\n",
    "    return N-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmm():\n",
    "    \"\"\"This class is intended to mimic the machine learning algorithms implemented in\n",
    "    Scikit-Learn. It's intended to make it as easy as possible to recover the\n",
    "    parameters pi, A, B of the HMM model as well as to project the observed data into the\n",
    "    hidden state space using the gamma distribution.\n",
    "    \n",
    "    Call the fit() method with observed data to recover the pi, A, B parameters from \n",
    "    an observed sequence of values. \"\"\"\n",
    "    \n",
    "    def __init__(self, num_hiddenstates = None, halting_criteria=1e-6, \n",
    "                 pi = None, A = None, B = None):\n",
    "        \"\"\"My initializer. If you know the number of hidden states, pass it\n",
    "        off via num_hiddenstates.\n",
    "        \n",
    "        halting_criteria is the parameter for specifying when to halt updating the values of pi, A, B.\n",
    "        \"\"\"\n",
    "        self.N = num_hiddenstates\n",
    "        self.halting_criteria = halting_criteria\n",
    "        self._called_fit = False\n",
    "        \n",
    "        self.pi = pi\n",
    "        self.A  = A\n",
    "        self.B  = B\n",
    "        if not self.N is None:\n",
    "            if self.A is None:\n",
    "                self.A = np.random.dirichlet(np.ones(self.N),size=self.N).transpose()\n",
    "                \n",
    "            if self.pi is None:\n",
    "                self.pi = np.random.dirichlet(np.ones(self.N),size=1).flatten()\n",
    "             \n",
    "    def fit(self, observed, verbose=False):\n",
    "        \"\"\" The method to call to find the values of pi, A, B from the observed data. \n",
    "        observed should be a list of observed values that are hashable.\n",
    "        \n",
    "        To map back from the index B[i,j] = Pr(Y=j | X= i) to the observed value Y,\n",
    "        use the dictionary self.inv_map[j] = observed value\"\"\"\n",
    "        \n",
    "        unique = list(set([y for y in observed]))\n",
    "        unique.sort()\n",
    "        self.map = dict([(y,t) for t,y in enumerate(unique)])\n",
    "        self.inv_map = dict([(t,y) for t,y in enumerate(unique)])\n",
    "        self.Y_t = np.array([self.map[y] for y in observed])\n",
    "        self.S = len(self.map)\n",
    "        self.T = len(self.Y_t)\n",
    "        \n",
    "        if self.N is None:\n",
    "            self.N =  determine_number_of_hidden_states(self.Y_t,\n",
    "                                                        halting_criteria=self.halting_criteria,\n",
    "                                                        verbose=verbose)\n",
    "                \n",
    "            self.pi, self.A, self.B = make_random(self.N,self.S)\n",
    "        else:\n",
    "            if self.B is None:\n",
    "                self.B = np.random.dirichlet(np.ones(self.S),size=self.N)\n",
    "                \n",
    "        reestimate_parameters(self.A,self.B,self.pi,self.Y_t,\n",
    "                              halting_criteria=self.halting_criteria)\n",
    "        self._called_fit = True\n",
    "    def transform(self):\n",
    "        \"\"\"Takes the observed data mapped to (Y_t) and then projected to the\n",
    "        gamma sequence. Also computes the log likelihood of the observed data.\n",
    "        Returns the gamma sequence. \"\"\"\n",
    "        \n",
    "        if not self._called_fit:\n",
    "            raise Exception(\"You need to call the fit method prior to calling the transform method.\")\n",
    "        \n",
    "        alpha = np.zeros((self.T,self.N))\n",
    "        beta  = np.zeros((self.T,self.N))\n",
    "        gamma = np.zeros((self.T,self.N))\n",
    "        sigma = np.zeros(self.T)\n",
    "        gammaoneshot(alpha,beta,gamma,self.pi,self.A,self.B,self.Y_t,sigma)\n",
    "        self.X_t = gamma\n",
    "        self.logp = compute_loglikelihood(sigma)\n",
    "        return gamma\n",
    "    def fit_transform(self,observed,verbose=False):\n",
    "        \"\"\"Fits the model to the observed data and returns the projection of the observed\n",
    "        data into the hidden state space via the gamma sequence, Y_t -> gamma_t\"\"\"\n",
    "        \n",
    "        self.fit(observed,verbose=verbose)\n",
    "        return self.transform()\n",
    "    \n",
    "    def compute_loglikelihood(self):\n",
    "        \"\"\" Computes log Pr(Y_0,...,Y_T| pi, A, B) \"\"\"\n",
    "        self.logp = compute_loglikelihood_parameters(self.pi,self.A,self.B,self.Y_t)\n",
    "        return self.logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "S = 26\n",
    "T = 10000\n",
    "\n",
    "pi,A,B =  make_random(N,S)\n",
    "X_t,Y_t = generate_data(pi,A,B,T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = !man bash\n",
    "sample_data = \"\".join(sample_data)\n",
    "sample_data = sample_data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_data = re.sub(\"\\ +\",\" \",sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = hmm(num_hiddenstates=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(new_sample_data[:1000],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compute_loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = hmm(halting_criteria=1e-4)\n",
    "my_model.fit(new_sample_data,verbose=True)\n",
    "print(my_model.compute_loglikelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_indices = my_model.B.argsort(axis=1)[:,-26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(biggest_indices.shape[0]):\n",
    "    print([my_model.inv_map[x] for x in biggest_indices[t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_model.transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
