{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "===\n",
    "\n",
    "I'm going to derive the equations for computing probabilities on hidden states given observed data for [Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries:\n",
    "\n",
    "I'm going to define my variables here:\n",
    "\n",
    "* $(Y_t)_{t=0}^{T}$ is the observed sequence\n",
    "* $(X_t)_{t=0}^T$ is the hidden sequence.\n",
    "* $A_{i\\rightarrow j} := Pr(X_{t+1} = j| X_t = i)$ is the transition matrix from one hidden state to another.\n",
    "* $B_i(j) := Pr(Y = j | X = i)$ is the probability distribution of the observed value as a function of the hidden state.\n",
    "* $\\pi(i) := Pr(X_0 = i)$ is the initial probability distribution on the hidden state.\n",
    "\n",
    "\n",
    "Here are the assumptions based on the Markovity of the hidden sequence:\n",
    "\n",
    "$$Pr(X_{t+1} = j| Y_0, \\ldots, Y_t, X_0=s_0,X_1=s_1, \\ldots,X_t = s_t) = Pr(X_{t+1} = j | X_t = s_t)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Pr((Y_s)_{s=t+1}^T | (Y_s)_{s=0}^t,X_0 = s_0,\\ldots,X_t=s_t) = Pr((Y_s)_{s=t}^T | X_t=s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goals:\n",
    "To be able to compute:\n",
    "    \n",
    "* $Pr(X_t = i | (Y_s)_{s=0}^T)$\n",
    "* $Pr(X_{t} =i, X_{t+1} = j | (Y_s)_{s=0}^T)$\n",
    "* Find a better model $\\pi, A, B$ that explains the observed data.\n",
    "* For a given set of parameters $\\pi, A, B$ find the most probable sequence of hidden states $(X_t)$ given\n",
    "the observed sequence $(Y_t)$.\n",
    "\n",
    "To do so I'll need to do some recursive computations. We begin:\n",
    "\n",
    "## Define: \n",
    "\n",
    "* $\\gamma_{t}(i) := Pr((Y_s)_{s=0}^T, X_t = i)$\n",
    "* $\\alpha_t(i) := Pr((Y_s)_{s=0}^t, X_t = i)$\n",
    "* $\\beta_{t}(i) := Pr((Y_s)_{s=t+1}^T| X_t=i)$\n",
    "\n",
    "## Relationship between $\\alpha, \\beta, \\gamma$:\n",
    "Here's the relationship between $\\alpha_t, \\beta_t, \\gamma_t$ :\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma_{t}(i) &:= Pr((Y_s)_{s=0}^T, X_t = i) \\\\\n",
    " &= Pr((Y_s)_{t+1}^T | X_t = i, (Y_s)_{s=0}^t ) Pr(X_t = i, (Y_s)_{s=0}^t )\\\\\n",
    " &= Pr((Y_s)_{t+1}^T | X_t = i) Pr(X_t = i, (Y_s)_{s=0}^t )\\\\\n",
    " &= \\beta_t(i)\\alpha_t(i) \\\\\n",
    " \\gamma_{t}(i) &= \\beta_t(i)\\alpha_t(i) \\\\\n",
    "\\end{align}  \n",
    "\n",
    "Since $\\gamma_T(i) \\equiv \\alpha_T(i)$ then $\\beta_T(i) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the relationship between $\\alpha_t, \\alpha_{t+1}$: \n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{t+1}(j) &= Pr((Y_s)_{s=0}^{t+1}, X_{t+1} = j) \\\\\n",
    "&= \\sum_i Pr( (Y_s)_{s=0}^{t+1}, X_{t+1} = j, X_t = i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}, X_{t+1} = j | (Y_s)_{s=0}^{t}, X_t = i)Pr( (Y_s)_{s=0}^{t}, X_t = i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}, X_{t+1} = j | X_t = i)\\alpha_t(i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}|X_{t+1} = j, X_t = i)Pr(X_{t+1} = j| X_t = i)\\alpha_t(i) \\\\\n",
    "&= \\sum_i Pr(Y_{t+1}|X_{t+1} = j)A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "&= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\alpha_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the relationship between $\\beta_t$ and $\\beta_{t+1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_t(j) & = Pr((Y_s)_{s=t+1}^T| X_t=j) \\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+1}^T,X_{t+1} = i| X_t=j) \\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+2}^T|Y_{t+1},X_{t+1} = i, X_t=j) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i Pr((Y_s)_{s=t+2}^T|X_{t+1} = i) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i \\beta_{t+1}(i) Pr(Y_{t+1},X_{t+1} = i| X_t = j)\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)Pr(Y_{t+1}|X_{t+1} = i, X_t = j)Pr(X_{t+1} = i | X_t = j)\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)Pr(Y_{t+1}|X_{t+1} = i)A_{j\\rightarrow i}\\\\\n",
    "&= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\beta_t(j) &= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Factors\n",
    "For any decent length observed sequence, the $\\alpha, \\beta, \\gamma$ sequences will quickly converge to 0 due to underflow issues in the finite precision arithmetic we have for computations.\n",
    "\n",
    "To avoid this underflow we instead compute conditional probabilities recursively which will not underflow as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional $\\alpha$\n",
    "\\begin{align}\n",
    "\\hat{\\alpha}_t(i) &:= Pr(X_t = i | Y_0,\\ldots, Y_t) \\\\\n",
    "&= \\frac{Pr(X_t = i, Y_0,\\ldots, Y_t)}{Pr(Y_0,\\ldots, Y_t)} \\\\\n",
    "&= \\frac{\\alpha_t(i)}{\\Lambda_t}, \\quad \\Lambda_t := Pr(Y_0,\\ldots, Y_t)\n",
    "\\end{align}\n",
    "$\\hat{\\alpha}_t$ satisfies the equation:\n",
    "\n",
    "$$\\sum_i \\hat{\\alpha}_t(i) = \\sum_i Pr(X_t = i | Y_0,\\ldots, Y_t) = 1$$\n",
    "In terms of the recursive computation of $\\alpha_{t+1}$  as a function of $\\alpha_t$ we have the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i) \\\\\n",
    "\\Lambda_{t+1}\\hat{\\alpha}_{t+1}(j) &= \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\Lambda_t\\hat{\\alpha}_t(i) \\\\\n",
    "\\hat{\\alpha}_{t+1}(j) &= \\frac{\\Lambda_{t}}{\\Lambda_{t+1}}\\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i) \\\\\n",
    "&= \\frac1{\\sigma_{t+1}} \\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i), \\quad \\sigma_{t+1} := \\frac{\\Lambda_{t+1}}{\\Lambda_{t}}\n",
    "\\end{align}\n",
    "We define $\\sigma_0 := Pr(Y_0)$, and we can recover $\\sigma_{t+1}$ recursively from the condition that $\\sum_j \\hat{\\alpha}_{t+1}(j) = 1$ which implies:\n",
    "\n",
    "$$\\sigma_{t+1} = \\sum_j\\sum_i B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)$$ \n",
    "\n",
    "and also:\n",
    "\n",
    "$$\\Lambda_t = \\prod_{s=0}^t \\sigma_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional $\\beta$\n",
    "\n",
    "We want to take advantage of how we defined $\\hat{\\alpha}$ so we can compute a $\\beta$ that does not underflow. Let's define $\\hat{\\gamma}_t(i)$ first:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\gamma}_t(i) &:= Pr(X_t = i | Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(X_t = i , Y_0,\\ldots, Y_T)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\alpha_t(i)\\beta_t(i)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\Lambda_t\\hat{\\alpha}_t(i)\\beta_t(i)}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\hat{\\alpha}_t(i)\\beta_t(i)\\frac{\\Lambda_t}{Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\hat{\\alpha}_t(i)\\hat{\\beta}_t(i) \\\\\n",
    "\\end{align}\n",
    "Now we can define $\\hat{\\beta}_t(i)$:\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_t(i) &:= \\frac{\\Lambda_t}{Pr(Y_0,\\ldots, Y_T)}\\beta_t(i) \\\\\n",
    "&= \\frac{\\prod_{s=0}^t \\sigma_s}{\\prod_{s=0}^T \\sigma_s}\\beta_t(i)\\\\\n",
    "&= \\frac1{\\prod_{s=t+1}^T \\sigma_s}\\beta_t(i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Lambda_{T} = Pr(Y_0,\\ldots,Y_T)$ we can deduce that $\\hat{\\beta}_T(i) = \\beta_T(i) = 1$ \n",
    "\n",
    "Now we can take advantage of the recursion on $\\beta_t$ to compute $\\hat{\\beta}_t$ as a function of $\\hat{\\beta}_{t+1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_t(j) &= \\sum_i  \\beta_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\left(\\prod_{s=t+1}^T \\sigma_s\\right) \\hat{\\beta}_t(j) &= \\left(\\prod_{s=t+2}^T\\sigma_s\\right)\\sum_i  \\hat{\\beta}_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\hat{\\beta}_t(j) &=\\frac{1}{\\sigma_{t+1}}\\sum_i  \\hat{\\beta}_{t+1}(i)B_i(Y_{t+1})A_{j\\rightarrow i}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the estimates of $A,B,\\pi$ based on the observations\n",
    "I want to apply [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) to improve the model for the observed data by updating the parameters $A,B,\\pi$. Some of the parameters are easy to re-estimate:\n",
    "\n",
    "$\\pi(i) := Pr(X_0 = i)$ gets updated to $\\pi'(i) = Pr(X_0=i | Y_0,\\ldots, Y_T) = \\hat{\\gamma}_0(i)$\n",
    "\n",
    "$B_i(y) := Pr(Y=y|X = i)$ gets updated to \n",
    "\n",
    "\\begin{align}\n",
    "B'_i(y) &:= Pr(Y=y | X=i, Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(Y=y, X=i, Y_0,\\ldots, Y_T)}{Pr(X=i, Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(Y=y, X=i| Y_0,\\ldots, Y_T)Pr( Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)Pr( Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(Y=y, X=i| Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{Pr(X=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{\\frac1T\\sum_{t=0}^T Pr(X_t=i| Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{Pr(X=i| Y=y, Y_0,\\ldots, Y_T)Pr(Y=y|Y_0,\\ldots, Y_T)}{\\frac1T\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\\n",
    "&= \\frac{\\frac1T\\sum_{t=0,Y_t=y}^T Pr(X=i| Y_0,\\ldots, Y_T)\\times 1}{\\frac1T\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\\n",
    "&= \\frac{\\sum_{t=0,Y_t=y}^T \\hat{\\gamma}_t(i)}{\\sum_{t=0}^T\\hat{\\gamma}_t(i)} \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to determine how to update $A$:\n",
    "\n",
    "$A_{i\\rightarrow j} = Pr(X_{new} = j | X_{old} = i)$ updates to \n",
    "\n",
    "\\begin{align}\n",
    "A'_{i \\rightarrow j} &= Pr(X_{new} = j | X_{old} = i,Y_0,\\ldots, Y_T) \\\\\n",
    "&= \\frac{Pr(X_{new} = j, X_{old} = i,Y_0,\\ldots, Y_T)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(X_{t+1} = j, X_{t} = i,Y_0,\\ldots, Y_T)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(X_{t+1} = j, Y_{t+1},\\ldots Y_T| X_{t} = i,Y_0,\\ldots, Y_t)Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+1},\\ldots Y_T| X_{t+1} = j, X_{t} = i,Y_0,\\ldots, Y_t)Pr(X_{t+1} = j| X_{t} = i,Y_0,\\ldots, Y_t) Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+1},\\ldots Y_T| X_{t+1} = j)Pr(X_{t+1} = j| X_{t} = i) Pr(X_{t} = i,Y_0,\\ldots, Y_t)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} Pr(Y_{t+2},\\ldots Y_T| X_{t+1} = j,Y_{t+1})Pr(Y_{t+1}|X_{t+1}=j)A_{i\\rightarrow j}\\alpha_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\beta_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\alpha_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s=t+2}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\left(\\prod_{s=0}^t \\sigma_s\\right)\\hat{\\alpha}_t(i)}{Pr(X_{old} = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\frac1{T-1}\\sum_{t=0}^{T-1}Pr(X_t = i,Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\frac1{T-1}\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\frac1{T-1}\\sum_{t=0}^{T-1}Pr(X_t = i|Y_0,\\ldots, Y_T)Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)Pr(Y_0,\\ldots, Y_T)} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1} \\left(\\prod_{s\\ne t+1}^T \\sigma_s\\right) \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)\\prod_{s=0}^T\\sigma_s} \\\\\n",
    "&= \\frac{\\sum_{t=0}^{T-1}  1/\\sigma_{t+1} \\hat{\\beta}_{t+1}(j)B_j(Y_{t+1})A_{i\\rightarrow j}\\hat{\\alpha}_t(i)}{\\sum_{t=0}^{T-1}\\hat{\\gamma}_t(i)} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\log\\left(Pr(Y_0,\\ldots, Y_T| \\pi,A,B)\\right)$\n",
    "Here's how we compute the log of the probability $Pr(Y_0,\\ldots, Y_T| \\pi,A,B)$:\n",
    "\n",
    "\\begin{align}\n",
    "Pr(Y_0,\\ldots, Y_T| \\pi,A,B) &= \\prod_{t=0}^T \\sigma_t \\\\\n",
    "\\log\\left(Pr(Y_0,\\ldots, Y_T| \\pi,A,B)\\right) &= \\sum_{t=0}^T \\log\\left(\\sigma_t\\right) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Most Probable Path\n",
    "Once the parameters $\\pi, A, B$ have been set and given the observed sequence $Y_0,\\ldots,Y_t$ how does one compute the most probable sequence of hidden states $X_0,\\ldots, X_T$? The technique we use is the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) or dynamic programming.\n",
    "\n",
    "\n",
    "Let $(n_t)_{t=1}^T$ denote a possible sequence of hidden states, $X_t = n_t$. How do we compute the probability of this occurrence?\n",
    "\n",
    "\\begin{align}\n",
    "Pr((n_t)|\\pi,A,B,(Y_t)) &= \\frac{Pr((n_t),\\pi,A,B,(Y_t))}{Pr(\\pi,A,B,(Y_t))}\\\\\n",
    "&= \\frac{Pr((n_t)_{t=2}^T,\\pi,A,B,(Y_t)_{t=2}^T)|Y_1,n_1)Pr(Y_1,n_1)}{Pr(\\pi,A,B,(Y_t))}\\\\\n",
    "&= \\frac{Pr((n_t)_{t=2}^T,\\pi,A,B,(Y_t)_{t=2}^T)|Y_1,n_1)Pr(Y_1|n_1)Pr(n_1)}{Pr(\\pi,A,B,(Y_t))}\\\\\n",
    "&= \\frac{B(n_1,Y_1)\\pi(n_1)\\prod_{t=2}^T A_{n_{t-1} \\rightarrow n_t}B(n_t,Y_t)}{Pr(\\pi,A,B,(Y_t))}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**\n",
    "To compute the most probable path we recursively compute the most probable path to time $t$.\n",
    "\n",
    "1. We have $N$ hidden states.\n",
    "2. For a time $t$ if we can find the $N$ most probable paths $p_{t,n}$ to the hidden state $X_t = 1, \\ldots, N$ we can extend them to the $N$ most probable paths $p_{t+1,n}$.\n",
    "\n",
    "\n",
    "\n",
    "$$p_{t+1,n} = \\max\\arg_{i}\\{p_{t,i}A_{i\\rightarrow n}B(n,Y_{t+1}) \\}$$\n",
    "3. Begin with $t=0$, $(p_{0,n} := \\pi(n)B(n,Y_0))_{n=1}^N$ and recurse through $t$ until we get to $t=T$ then the\n",
    "most probable path is given by $\\max arg_{i} \\{ p_{T,i} \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of hidden states\n",
    "I can use [Wilks' theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem) to determine the number of hidden states as follows:\n",
    "\n",
    "1. Assume  $n$ hidden states. A model with $n$ hidden states and $S$\n",
    " observable states requires $n-1 + n(S-1) + n(n-1) = n(1 + S-1 + n -1) -1 = n(n + S -1) - 1 $ parameters.\n",
    "2. Use Baum-Welch's algorithm to hill climb to the best model it can find. \n",
    "3. Compute the log likelihood of the observed data , $S_{null} :=\\log Pr(Y_0,\\ldots, Y_T| \\pi, A,B)$.\n",
    "4. Assume now $n+1$ hidden states and as above recover the MLE parameters and  log likelihood, $S_{alternative}$.\n",
    "5. Compute the statistic $\\Lambda := 2\\times (S_{alternative} - S_{null})$. This statistic will be a sample from a Chi-squared distribution with $df = (n+1)(n+1 +S -1) - n(n+S-1) = n^2 + n + nS + S - n^2 - nS + n = 2n +S$ degrees of freedom. \n",
    "6. Compute the $p$ value $p:=Pr(\\xi \\ge \\Lambda)$. where $\\xi$ is a random variable of type Chi-Square distribution with $df = 2n +S$ degrees of freedom. If $p$ is below my threshold stick with alternative hypothesis (1 more hidden state) otherwise stop at $n$ hidden states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "I want to demonstrate that this derivation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random(N,S):\n",
    "    \"\"\"generates a random pi,A,B triplet for me.\"\"\"\n",
    "\n",
    "    pi = np.random.dirichlet(np.ones(N))\n",
    "    A  = np.random.dirichlet(np.ones(N),N).transpose()\n",
    "    B  = np.random.dirichlet(np.ones(S),N)\n",
    "\n",
    "    return pi,A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(pi,A,B,T = 100):\n",
    "    \"\"\"Generates an observed sequence of length T\"\"\"\n",
    "    N, S = B.shape\n",
    "    \n",
    "    # Generate hidden path\n",
    "    X_t = np.zeros(T,dtype=np.int32)\n",
    "    X_t[0] = np.argwhere(np.random.multinomial(1,pi,size=1).flatten()==1).flatten()[0]\n",
    "    for t in range(1,T):\n",
    "        X_t[t] = np.argwhere(np.random.multinomial(1,A[:,X_t[t-1]],size=1).flatten()==1).flatten()[0]\n",
    "\n",
    "    # Generate observed sequence\n",
    "    Y_t = np.zeros(T,dtype=np.int32)\n",
    "    for t in range(T):\n",
    "        Y_t[t] = np.argwhere(np.random.multinomial(1,B[X_t[t]],size=1).flatten()==1).flatten()[0]\n",
    "\n",
    "    return X_t,Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def alphapass(alpha,sigma,A,B,pi,observed):\n",
    "    \"\"\"\n",
    "    alpha(t,i) = Pr(hidden_t = i | obs_1,...,obs_t)\n",
    "    beta(t,i)  = Pr(obs_t+1,...,obs_T| hidden_t = i)*Pr(o_1,...,o_t)/Pr(o_1,...,o_T)\n",
    "    gamma(t,i)  = Pr(hidden_t = i | obs_1,...,obs_T)\n",
    "    A(j,i) == Pr( hidden i -> hidden j)\n",
    "    B(i,j) == Pr( observed j | hidden i)\n",
    "    \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    # initial iteration\n",
    "    alpha[0] = pi\n",
    "    alpha[0] = alpha[0]*B[:,observed[0]]\n",
    "    sigma[0] = alpha[0].sum()\n",
    "    alpha[0] /= sigma[0]\n",
    "\n",
    "    # now do the remainder\n",
    "    for t in range(1,T):\n",
    "        alpha[t] = np.dot(A,alpha[t-1])\n",
    "        alpha[t] *= B[:,observed[t]]\n",
    "        sigma[t] = alpha[t].sum()\n",
    "        alpha[t] /= sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def betapass(beta,sigma,A,B,observed):\n",
    "    \"\"\"\n",
    "    A(j,i) == Pr( hidden i -> hidden j)\n",
    "    B(i,j) == Pr( observed j | hidden i)\n",
    "    \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    At = A.transpose()\n",
    "\n",
    "    # initial iteration\n",
    "    beta[-1] = 1.\n",
    "\n",
    "    # now do the remainder\n",
    "    for t in range(T-1,0,-1):\n",
    "        beta[t-1] = np.dot(At,beta[t]*B[:,observed[t]])/sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def gammapass(alpha,beta,gamma):\n",
    "    gamma[:,:] = alpha*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def update_B(B,gamma, observed):\n",
    "    \"\"\" Updates belief on B \"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "\n",
    "    B_new = np.zeros((N,S))\n",
    "\n",
    "    for t in range(T):\n",
    "        B_new[:,observed[t]] += gamma[t]\n",
    "\n",
    "    B_new = B_new/B_new.sum(axis=1).reshape(-1,1)\n",
    "    B[:] = B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi(pi,gamma):\n",
    "    \"\"\" Updates belief on pi \"\"\"\n",
    "    pi[:] = gamma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def digammapass(alpha,beta,gamma,observed,sigma,digamma,A,B):\n",
    "    \"\"\"\n",
    "    Computes an updated version of A based on the observed sequence of data.\n",
    "    \n",
    "    alpha(t,i) = Pr(hidden_t = i | obs_1,...,obs_t)\n",
    "    beta(t,i)  = Pr(obs_t+1,...,obs_T| hidden_t = i)*Pr(o_1,...,o_t)/Pr(o_1,...,o_T)\n",
    "    gamma(t,i)  = Pr(hidden_t = i | obs_1,...,obs_T)\n",
    "    A(j,i) = Pr( hidden i -> hidden j)\n",
    "    B(i,j) = Pr( observed j | hidden i)\n",
    "\n",
    "    Reestimates Pr(X=i -> X=j) via averaging\n",
    "    Pr(x_t=i,x_t+1=j|o_1,...,o_T) = alpha_t(i)A_jibeta_{t+1}(j)B(j,o_{t+1})\n",
    "    \"\"\"\n",
    "\n",
    "    T = alpha.shape[0]\n",
    "    N,S = B.shape\n",
    "    \n",
    "    digamma[:] = 0\n",
    "    digammatemp = np.zeros((N,N))\n",
    "    for t in range(0,T-1):\n",
    "        temp_i = (alpha[t]/sigma[t+1]).reshape(1,-1)*A\n",
    "        temp_j = (B[:,observed[t+1]]*beta[t+1]).reshape(-1,1)\n",
    "        digammatemp = temp_i*temp_j\n",
    "        digamma[:] += digammatemp\n",
    "\n",
    "    # rescale columns as \\sum_j A_{j,i} == 1\n",
    "    digamma[:] = digamma/digamma.sum(axis=0).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(alpha,beta,gamma, pi, A,B,observed,sigma):\n",
    "    \"\"\" Updates pi, A,B from observed data. \"\"\"\n",
    "\n",
    "    N,S = B.shape\n",
    "    digamma = np.zeros((N,N))\n",
    "    digammapass(alpha, beta, gamma, observed,sigma,digamma, A,B)\n",
    "\n",
    "    update_B(B,gamma,observed)\n",
    "    update_pi(pi,gamma)\n",
    "\n",
    "    A[:] = digamma[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma):\n",
    "    \"\"\" computes alpha, beta, and gamma directly for me. \"\"\"\n",
    "\n",
    "    alphapass(alpha,sigma,A,B,pi,observed)\n",
    "    betapass(beta,sigma,A,B,observed)\n",
    "    gammapass(alpha,beta,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglikelihood(sigma):\n",
    "    \"\"\"Computes Pr(Y_0,...,Y_T| pi, A, B) from (sigma_t)\"\"\"\n",
    "    return log(sigma).sum()\n",
    "\n",
    "def compute_loglikelihood_parameters(pi,A,B,observed):\n",
    "    \"\"\"Computes Pr(Y_0,...,Y_T| pi, A, B) from (pi, A, B)\"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "    alpha = zeros((T,N))\n",
    "    beta  = zeros((T,N))\n",
    "    gamma = zeros((T,N))\n",
    "    sigma = zeros(T)\n",
    "    \n",
    "    gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "    score = compute_loglikelihood(sigma)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reestimate_parameters(A,B,pi,observed,\n",
    "                          halting_criteria=1e-6,debug=False):\n",
    "    \"\"\"re-estimates pi, A, and B from the observed sequence.\"\"\"\n",
    "    N, S = B.shape\n",
    "    T = observed.shape[0]\n",
    "    alpha = zeros((T,N))\n",
    "    beta  = zeros((T,N))\n",
    "    gamma = zeros((T,N))\n",
    "    sigma = zeros(T)\n",
    "    \n",
    "    current_score = -np.inf\n",
    "    gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "    new_score = compute_loglikelihood(sigma)\n",
    "    diff = new_score - current_score\n",
    "    while diff > halting_criteria:\n",
    "        current_score = new_score\n",
    "        update_parameters(alpha,beta,gamma, pi, A,B,observed,sigma)\n",
    "        gammaoneshot(alpha,beta,gamma,pi,A,B,observed,sigma)\n",
    "        new_score = compute_loglikelihood(sigma)\n",
    "        diff = new_score - current_score\n",
    "        if debug:\n",
    "            print(\"diff = \", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_number_of_hidden_states(Y_t,halting_criteria=1e-6,verbose=False):\n",
    "    \"\"\"Uses Wilks' theorem to determine the number of hidden states based on\n",
    "    the observed data (Y_t). It returns the number of hidden states\"\"\"\n",
    "    S = Y_t.max() + 1\n",
    "    N = 1\n",
    "    my_pi, my_A, my_B = make_random(N,S)\n",
    "\n",
    "    reestimate_parameters(my_A,my_B, my_pi,Y_t,halting_criteria=halting_criteria)\n",
    "    S_alternative =compute_loglikelihood_parameters(my_pi, my_A, my_B, Y_t)\n",
    "\n",
    "    p = 0.0\n",
    "    while p < 0.01:\n",
    "        df = 2*N + S\n",
    "        N += 1\n",
    "        my_pi, my_A, my_B = make_random(N,S)\n",
    "    \n",
    "        S_null = S_alternative\n",
    "        reestimate_parameters(my_A,my_B, my_pi,Y_t,halting_criteria=halting_criteria)\n",
    "        S_alternative =compute_loglikelihood_parameters(my_pi, my_A, my_B, Y_t)\n",
    "\n",
    "        model =st.chi2(df = df)\n",
    "        lam = 2*(S_alternative - S_null)\n",
    "        p = model.sf(lam)\n",
    "        if verbose:\n",
    "            print(\"N=\",N,\"df: \", df, \"Lamda: \", lam, \"p: \", p)\n",
    "    return N-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(pi,A,B,Y,verbose=False):\n",
    "    \"\"\"Computes the most probable path for the hidden sequence (X_t)\n",
    "    returns the score, and path as a list of hidden state values.\"\"\"\n",
    "  \n",
    "    N = A.shape[0]\n",
    "    T = len(Y)\n",
    "    \n",
    "    logA = np.log(A)\n",
    "    logB = np.log(B)\n",
    "    logpi = np.log(pi)\n",
    "    # start with t=0\n",
    "    paths = [[logpi[n]+logB[n,Y[0]],[n]] for n in range(N)]\n",
    "    \n",
    "    # now recurse until t = T-1\n",
    "    for t in range(1, T):\n",
    "        new_paths = [[] for n in range(N)]\n",
    "        for n in range(N):\n",
    "            scores = np.array([ paths[i][0] + logA[n,i] for i in range(N)])\n",
    "            i = scores.argmax()\n",
    "            new_score = scores.max() + logB[n,Y[t]]\n",
    "            new_paths[n] = [new_score,paths[i][1] + [n]]\n",
    "\n",
    "        paths = new_paths\n",
    "        \n",
    "    total_scores = np.array([x[0] for x in paths])\n",
    "    best_score = total_scores.max()\n",
    "    best_path = paths[total_scores.argmax()][1]\n",
    "    return best_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmm():\n",
    "    \"\"\"This class is intended to mimic the machine learning algorithms implemented in\n",
    "    Scikit-Learn. It's intended to make it as easy as possible to recover the\n",
    "    parameters pi, A, B of the HMM model as well as to project the observed data into the\n",
    "    hidden state space using the gamma distribution.\n",
    "    \n",
    "    Call the fit() method with observed data to recover the pi, A, B parameters from \n",
    "    an observed sequence of values. \"\"\"\n",
    "    \n",
    "    def __init__(self, num_hiddenstates = None, halting_criteria=1e-6, \n",
    "                 pi = None, A = None, B = None):\n",
    "        \"\"\"My initializer. If you know the number of hidden states, pass it\n",
    "        off via num_hiddenstates.\n",
    "        \n",
    "        halting_criteria is the parameter for specifying when to halt updating the values of pi, A, B.\n",
    "        \"\"\"\n",
    "        self.N = num_hiddenstates\n",
    "        self.halting_criteria = halting_criteria\n",
    "        self._called_fit = False\n",
    "        \n",
    "        self.pi = pi\n",
    "        self.A  = A\n",
    "        self.B  = B\n",
    "        if not self.N is None:\n",
    "            if self.A is None:\n",
    "                self.A = np.random.dirichlet(np.ones(self.N),size=self.N).transpose()\n",
    "                \n",
    "            if self.pi is None:\n",
    "                self.pi = np.random.dirichlet(np.ones(self.N),size=1).flatten()\n",
    "             \n",
    "    def fit(self, observed, verbose=False):\n",
    "        \"\"\" The method to call to find the values of pi, A, B from the observed data. \n",
    "        observed should be a list of observed values that are hashable.\n",
    "        \n",
    "        To map back from the index B[i,j] = Pr(Y=j | X= i) to the observed value Y,\n",
    "        use the dictionary self.inv_map[j] = observed value\"\"\"\n",
    "        \n",
    "        unique = list(set([y for y in observed]))\n",
    "        unique.sort()\n",
    "        self.map = dict([(y,t) for t,y in enumerate(unique)])\n",
    "        self.inv_map = dict([(t,y) for t,y in enumerate(unique)])\n",
    "        self.Y_t = np.array([self.map[y] for y in observed])\n",
    "        self.S = len(self.map)\n",
    "        self.T = len(self.Y_t)\n",
    "        \n",
    "        if self.N is None:\n",
    "            self.N =  determine_number_of_hidden_states(self.Y_t,\n",
    "                                                        halting_criteria=self.halting_criteria,\n",
    "                                                        verbose=verbose)\n",
    "                \n",
    "            self.pi, self.A, self.B = make_random(self.N,self.S)\n",
    "        else:\n",
    "            if self.B is None:\n",
    "                self.B = np.random.dirichlet(np.ones(self.S),size=self.N)\n",
    "                \n",
    "        reestimate_parameters(self.A,self.B,self.pi,self.Y_t,\n",
    "                              halting_criteria=self.halting_criteria)\n",
    "        self._called_fit = True\n",
    "    def transform(self):\n",
    "        \"\"\"Takes the observed data mapped to (Y_t) and then projected to the\n",
    "        gamma sequence. Also computes the log likelihood of the observed data.\n",
    "        Returns the gamma sequence. \"\"\"\n",
    "        \n",
    "        if not self._called_fit:\n",
    "            raise Exception(\"You need to call the fit method prior to calling the transform method.\")\n",
    "        \n",
    "        alpha = np.zeros((self.T,self.N))\n",
    "        beta  = np.zeros((self.T,self.N))\n",
    "        gamma = np.zeros((self.T,self.N))\n",
    "        sigma = np.zeros(self.T)\n",
    "        gammaoneshot(alpha,beta,gamma,self.pi,self.A,self.B,self.Y_t,sigma)\n",
    "        self.logp = compute_loglikelihood(sigma)\n",
    "        return gamma\n",
    "    def fit_transform(self,observed,verbose=False):\n",
    "        \"\"\"Fits the model to the observed data and returns the projection of the observed\n",
    "        data into the hidden state space via the gamma sequence, Y_t -> gamma_t\"\"\"\n",
    "        \n",
    "        self.fit(observed,verbose=verbose)\n",
    "        return self.transform()\n",
    "    \n",
    "    def viterbi(self):\n",
    "        \"\"\"Computes the most probable sequence of hidden states and returns the sequence \"\"\"\n",
    "\n",
    "        best_score, path = viterbi_algorithm(self.pi, self.A,self.B,self.Y_t)\n",
    "        return path\n",
    "    \n",
    "    def compute_loglikelihood(self):\n",
    "        \"\"\" Computes log Pr(Y_0,...,Y_T| pi, A, B) \"\"\"\n",
    "        self.logp = compute_loglikelihood_parameters(self.pi,self.A,self.B,self.Y_t)\n",
    "        return self.logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage:\n",
    "\n",
    "### Constructing data for a HMM:\n",
    "How to construct data for a Hidden Markove Model with  3 hidden states, 26 observable states and an observed sequence of 10,000 symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "S = 26\n",
    "T = 10000\n",
    "\n",
    "pi,A,B =  make_random(N,S)\n",
    "X_t,Y_t = generate_data(pi,A,B,T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing a sequence\n",
    "\n",
    "Here we're going to try to model with a sequence of text as being generated by a 2 hidden state HMM. The hidden states are *vowels* and *consonants*.\n",
    "Part of the issue here though is that the content also includes white space and non-alphabetic characters. How will the HMM assign them to clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = \"\"\"bash(1) general commands manual bash(1)name bash - gnu bourne-again shellsynopsis bash [options] \n",
    "[command_string | file]copyright bash is copyright (c) 1989-2018 by the free software foundation, inc.description \n",
    "bash is an sh-compatible command language interpreter that executes commands read from the standard input or from \n",
    "a file. bash also incor‐ porates useful features from the korn and c shells (ksh and csh). bash is intended to be \n",
    "a conformant implementation of the shell and utilities portion of the ieee posix specification (ieee standard \n",
    "1003.1). bash can be configured to be posix-conformant by default.options all of the single-character shell options \n",
    "documented in the description of the set builtin command, including -o, can be used as options when the shell is \n",
    "invoked. in addition, bash interprets the following op‐ tions when it is invoked: -c if the -c option is present, \n",
    "then commands are read from the first non-option argument command_string. if there are argu‐ ments after the \n",
    "command_string, the first argument is as‐ signed to $0 and any remaining arguments are assigned to the positional \n",
    "parameters. the assignment to $0 sets the name of the shell, which is used in warning and error messages. -i if \n",
    "the -i option is present, the shell is interactive. -l make bash act as if it had been invoked as a login shell \n",
    "(see invocation below). -r if the -r option is present, the shell becomes restricted (see restricted shell below). \n",
    "-s if the -s option is present, or if no arguments remain after option processing, then commands are read from \n",
    "the standard input. this option allows the positional parameters to be set when invoking an interactive shell or \n",
    "when reading input through a pipe. -v print shell input lines as they are read. -x print commands and their \n",
    "arguments as they are executed. -d a list of all double-quoted strings preceded by $ is printed on the standard \n",
    "output. these are the strings that are sub‐ ject to language translation when the current locale is no\"\"\"\n",
    "\n",
    "sample_data = \" \".join(sample_data.split()) # Remove double white spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and fit it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_model = hmm(num_hiddenstates=3)\n",
    "my_model.fit(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"log likelihood of observed data:\",my_model.compute_loglikelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project the observed sequence to the hidden state space, $Y_t \\rightarrow \\gamma_t(\\_)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = my_model.transform()\n",
    "print(\"Y_t\\tProjection(Y_t)\")\n",
    "for t in range(20):\n",
    "    print(\"{0}\\t {1:6.4f} {2:6.4f} {3:6.4f}\".format(sample_data[t],*gamma[t]))\n",
    "#print(gamma[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most probable sequence of hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = my_model.viterbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the most probable path vs. the sequence of most probable hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most probable X_t sequence according to Viterbi vs. Gamma argmax\")\n",
    "print(path[:40])\n",
    "print(list(gamma[:40].argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreements =  (np.array(path) == gamma.argmax(axis=1)).sum()\n",
    "print(\"Percentage agreement: {0:6.2f}%\".format(100*agreements/gamma.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster symbols using the $B$ matrix. Which symbols are *vowels* and which are *consonants*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = my_model.B.argmax(axis=0)\n",
    "\n",
    "groups = [ [] for t in range(my_model.B.shape[0])]\n",
    "for i in range(my_model.B.shape[1]):\n",
    "    letter = my_model.inv_map[i]\n",
    "    groups[assignments[i]].append(letter)\n",
    "    \n",
    "for t in range(my_model.B.shape[0]):\n",
    "    groups[t].sort()\n",
    "\n",
    "print(\"Letter clusters:\")\n",
    "for t in range(my_model.B.shape[0]):\n",
    "    print(\"Group {0}: \".format(t),\"\".join(groups[t]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
